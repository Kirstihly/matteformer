{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1af046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import toml\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import utils\n",
    "from   utils import CONFIG\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1027cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_inference(model, image_dict):\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        image, trimap = image_dict['image'], image_dict['trimap']\n",
    "\n",
    "        image = image.cuda()\n",
    "        trimap = trimap.cuda()\n",
    "\n",
    "        # run model\n",
    "        pred = model(image, trimap)\n",
    "        alpha_pred_os1, alpha_pred_os4, alpha_pred_os8 = pred['alpha_os1'], pred['alpha_os4'], pred['alpha_os8']\n",
    "\n",
    "        # refinement\n",
    "        alpha_pred = alpha_pred_os8.clone().detach()\n",
    "        weight_os4 = utils.get_unknown_tensor_from_pred(alpha_pred, rand_width=CONFIG.model.self_refine_width1, train_mode=False)\n",
    "        alpha_pred[weight_os4>0] = alpha_pred_os4[weight_os4>0]\n",
    "        weight_os1 = utils.get_unknown_tensor_from_pred(alpha_pred, rand_width=CONFIG.model.self_refine_width2, train_mode=False)\n",
    "        alpha_pred[weight_os1>0] = alpha_pred_os1[weight_os1>0]\n",
    "\n",
    "        h, w = image_dict['alpha_shape']\n",
    "        alpha_pred = alpha_pred[0, 0, ...].data.cpu().numpy() * 255\n",
    "        alpha_pred = alpha_pred.astype(np.uint8)\n",
    "\n",
    "        alpha_pred[np.argmax(trimap.cpu().numpy()[0], axis=0) == 0] = 0.0\n",
    "        alpha_pred[np.argmax(trimap.cpu().numpy()[0], axis=0) == 2] = 255.\n",
    "\n",
    "        alpha_pred = alpha_pred[32:h+32, 32:w+32]\n",
    "\n",
    "        return alpha_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f522e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_tensor_dict(image_path, trimap_path):\n",
    "    # read images\n",
    "    image = cv2.imread(image_path)\n",
    "    trimap = cv2.imread(trimap_path, 0)\n",
    "\n",
    "    sample = {'image': image, 'trimap':trimap, 'alpha_shape':(image.shape[0], image.shape[1])}\n",
    "\n",
    "    # reshape\n",
    "    h, w = sample[\"alpha_shape\"]\n",
    "    \n",
    "    if h % 32 == 0 and w % 32 == 0:\n",
    "        padded_image = np.pad(sample['image'], ((32,32), (32, 32), (0,0)), mode=\"reflect\")\n",
    "        padded_trimap = np.pad(sample['trimap'], ((32,32), (32, 32)), mode=\"reflect\")\n",
    "\n",
    "        sample['image'] = padded_image\n",
    "        sample['trimap'] = padded_trimap\n",
    "\n",
    "    else:\n",
    "        target_h = 32 * ((h - 1) // 32 + 1)\n",
    "        target_w = 32 * ((w - 1) // 32 + 1)\n",
    "        pad_h = target_h - h\n",
    "        pad_w = target_w - w\n",
    "        padded_image = np.pad(sample['image'], ((32,pad_h+32), (32, pad_w+32), (0,0)), mode=\"reflect\")\n",
    "        padded_trimap = np.pad(sample['trimap'], ((32,pad_h+32), (32, pad_w+32)), mode=\"reflect\")\n",
    "\n",
    "        sample['image'] = padded_image\n",
    "        sample['trimap'] = padded_trimap\n",
    "\n",
    "    # ImageNet mean & std\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "    # convert GBR images to RGB\n",
    "    image, trimap = sample['image'][:,:,::-1], sample['trimap']\n",
    "\n",
    "    # swap color axis\n",
    "    image = image.transpose((2, 0, 1)).astype(np.float32)\n",
    "\n",
    "    # trimap configuration\n",
    "    padded_trimap[padded_trimap < 85] = 0\n",
    "    padded_trimap[padded_trimap >= 170] = 2\n",
    "    padded_trimap[padded_trimap >= 85] = 1\n",
    "\n",
    "    # normalize image\n",
    "    image /= 255.\n",
    "\n",
    "    # to tensor\n",
    "    sample['image'], sample['trimap'] = torch.from_numpy(image), torch.from_numpy(trimap).to(torch.long)\n",
    "    sample['image'] = sample['image'].sub_(mean).div_(std)\n",
    "\n",
    "    # trimap to one-hot 3 channel\n",
    "    sample['trimap'] = F.one_hot(sample['trimap'], num_classes=3).permute(2, 0, 1).float()\n",
    "\n",
    "    # add first channel\n",
    "    sample['image'], sample['trimap'] = sample['image'][None, ...], sample['trimap'][None, ...]\n",
    "\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d3e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--config', type=str, default='config/MatteFormer_Composition1k.toml')\n",
    "parser.add_argument('--checkpoint', type=str, default='pretrained/best_model.pth', help=\"path of checkpoint\")\n",
    "parser.add_argument('--image-dir', type=str, default='/home/leyinghu/Documents/data/Real_data0720/images', help=\"input image dir\")\n",
    "parser.add_argument('--mask-dir', type=str, default='/home/leyinghu/Documents/data/Real_data0720/trimaps', help=\"input trimap dir\")\n",
    "parser.add_argument('--trimap-dir', type=str, default='/home/leyinghu/Documents/data/Real_data0720/trimaps', help=\"input trimap dir\")\n",
    "parser.add_argument('--output', type=str, default='pred/', help=\"output dir\")\n",
    "\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c6c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.config) as f:\n",
    "    utils.load_config(toml.load(f))\n",
    "\n",
    "# Check if toml config file is loaded\n",
    "if CONFIG.is_default:\n",
    "    raise ValueError(\"No .toml config loaded.\")\n",
    "\n",
    "utils.make_dir(os.path.join(args.output, 'pred_alpha'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ebfbdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leyinghu/.local/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/leyinghu/.local/lib/python3.8/site-packages/torch/nn/functional.py:4037: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20] inference done : pred/pred_alpha/20.png\n",
      "[1/20] inference done : pred/pred_alpha/12.png\n",
      "[2/20] inference done : pred/pred_alpha/5.png\n",
      "[3/20] inference done : pred/pred_alpha/4.png\n",
      "[4/20] inference done : pred/pred_alpha/15.png\n",
      "[5/20] inference done : pred/pred_alpha/19.png\n",
      "[6/20] inference done : pred/pred_alpha/2.png\n",
      "[7/20] inference done : pred/pred_alpha/14.png\n",
      "[8/20] inference done : pred/pred_alpha/13.png\n",
      "[9/20] inference done : pred/pred_alpha/7.png\n",
      "[10/20] inference done : pred/pred_alpha/10.png\n",
      "[11/20] inference done : pred/pred_alpha/6.png\n",
      "[12/20] inference done : pred/pred_alpha/8.png\n",
      "[13/20] inference done : pred/pred_alpha/18.png\n",
      "[14/20] inference done : pred/pred_alpha/9.png\n",
      "[15/20] inference done : pred/pred_alpha/16.png\n",
      "[16/20] inference done : pred/pred_alpha/17.png\n",
      "[17/20] inference done : pred/pred_alpha/11.png\n",
      "[18/20] inference done : pred/pred_alpha/1.png\n",
      "[19/20] inference done : pred/pred_alpha/3.png\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = networks.get_generator(is_train=False)\n",
    "model.cuda()\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = torch.load(args.checkpoint)\n",
    "model.load_state_dict(utils.remove_prefix_state_dict(checkpoint['state_dict']), strict=True)\n",
    "\n",
    "# inference\n",
    "model = model.eval()\n",
    "\n",
    "for i, image_name in enumerate(os.listdir(args.image_dir)):\n",
    "\n",
    "    # assume image and mask have the same file name\n",
    "    image_path = os.path.join(args.image_dir, image_name)\n",
    "    trimap_path = os.path.join(args.trimap_dir, image_name)\n",
    "\n",
    "    image_dict = generator_tensor_dict(image_path, trimap_path)\n",
    "    alpha_pred = single_inference(model, image_dict)\n",
    "\n",
    "    # save images\n",
    "    _im = cv2.imread(image_path)\n",
    "    _tr = cv2.imread(trimap_path)\n",
    "    _al = cv2.cvtColor(alpha_pred, cv2.COLOR_GRAY2RGB)\n",
    "    h, w, c = _al.shape\n",
    "\n",
    "    canvas = np.zeros((h, w*3, c))\n",
    "    canvas[:, w*0:w*1, :] = _im\n",
    "    canvas[:, w*1:w*2, :] = _tr\n",
    "    canvas[:, w*2:w*3, :] = _al\n",
    "\n",
    "    cv2.imwrite(os.path.join(args.output, 'pred_alpha', image_name), _al)\n",
    "    print('[{}/{}] inference done : {}'.format(i, len(os.listdir(args.image_dir)), os.path.join(args.output, 'pred_alpha', image_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45003e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
